{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "convertible-grounds",
   "metadata": {},
   "source": [
    "# Classifier for Inception Score\n",
    "\n",
    "by Búgvi Benjamin Magnussen and Nikolaj Bläser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "herbal-integration",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "robust-patient",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "central-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train, Y_train), (X_test_validation, Y_test_validation) = mnist\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "Y_test_validation = tf.keras.utils.to_categorical(Y_test_validation)\n",
    "\n",
    "# Dividing the second dataset into test and validation sets\n",
    "X_validation = X_test_validation[len(X_test_validation)//2:]\n",
    "X_test = X_test_validation[:len(X_test_validation)//2]\n",
    "\n",
    "Y_validation = Y_test_validation[len(Y_test_validation)//2:]\n",
    "Y_test = Y_test_validation[:len(Y_test_validation)//2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neither-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images): \n",
    "  return (images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "antique-frank",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_validation = normalize(X_validation)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-airline",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = X_train[0]\n",
    "IMAGE_WIDTH = image.shape[0]\n",
    "IMAGE_HEIGHT = image.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-guitar",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "train = tf.data.Dataset.from_tensor_slices(tuple((X_train, Y_train))).shuffle(len(X_train)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-prototype",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-advertiser",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is from tensorflow.contrib.layers.maxout\n",
    "import tensorflow as tf\n",
    "def maxout(inputs, num_units, axis=-1):\n",
    "  '''\n",
    "  inputs: Tensor input\n",
    "  num_units: The num of unit keeped after amxout\n",
    "  axis: The dimension max op performed\n",
    "  scope: Optional scope for variable_scope\n",
    "     Note: This is a slightly modified version. Replaced some unused API functions\n",
    "  '''\n",
    "  shape = inputs.get_shape().as_list()\n",
    "  num_channels = shape[axis]\n",
    "  if num_channels % num_units:\n",
    "    raise ValueError('number of features({}) is not '\n",
    "                      'a multiple of num_units({})'.format(\n",
    "                          num_channels, num_units))\n",
    "  shape[axis] = -1\n",
    "  shape += [num_channels // num_units]\n",
    "\n",
    "  # Dealing with batches with arbitrary sizes\n",
    "  for i in range(len(shape)): # This is used to handle the case where None is included in the shape\n",
    "    if shape[i] is None:\n",
    "      shape[i] = tf.shape(inputs)[i]\n",
    "  outputs = tf.reduce_max( tf.reshape(inputs, shape), -1)\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-albania",
   "metadata": {},
   "source": [
    "# Layer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suited-investor",
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseRelu(inputs, weights, bias, leaky_relu_alpha = 0.2):\n",
    "  return tf.nn.leaky_relu(tf.nn.bias_add(tf.matmul(inputs, weights), bias), alpha=leaky_relu_alpha)\n",
    "\n",
    "def denseMaxout(inputs, weights, bias, num_of_units=2, dropout_rate=0.5):\n",
    "  z = tf.nn.bias_add(tf.matmul(inputs, weights), bias)\n",
    "  z_dropout = tf.nn.dropout(z, rate=dropout_rate)\n",
    "  return maxout(z_dropout, num_of_units)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proprietary-landing",
   "metadata": {},
   "source": [
    "# Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-lafayette",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.glorot_normal()\n",
    "bias_initializer = tf.initializers.zeros()\n",
    "\n",
    "def get_biases(n_units, name):\n",
    "  return tf.Variable(bias_initializer(n_units, dtype=tf.float64), name = name, trainable = True, dtype=tf.float64)\n",
    "\n",
    "\n",
    "def get_weights(shape, name):\n",
    "  return tf.Variable(initializer(shape, dtype=tf.float64), name = name, trainable = True, dtype=tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "destroyed-parallel",
   "metadata": {},
   "outputs": [],
   "source": [
    "biases = [\n",
    "  get_biases(1200, 'bias0'),\n",
    "  get_biases(1200, 'bias1'),\n",
    "  get_biases(10, 'bias2')\n",
    "]\n",
    "\n",
    "weights = [\n",
    "  get_weights([image.shape[0] * image.shape[1], 1200], 'weights0'),\n",
    "  get_weights([240, 1200], 'weights1'),\n",
    "  get_weights([240, 10], 'weights2'),\n",
    "]\n",
    "\n",
    "parameters = weights + biases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-destination",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "\n",
    "@tf.function\n",
    "def classifier(x):\n",
    "  x = tf.cast(x, dtype=tf.float64)\n",
    "  x = tf.reshape(x, shape=[x.shape[0], x.shape[1] * x.shape[2]])\n",
    "  d1 = denseMaxout(x, weights[0], biases[0], num_of_units=240, dropout_rate=dropout_rate)\n",
    "  d2 = denseMaxout(d1, weights[1], biases[1], num_of_units=240, dropout_rate=dropout_rate)\n",
    "  return tf.nn.softmax(tf.nn.bias_add(tf.matmul(d2, weights[2]), biases[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-serum",
   "metadata": {},
   "source": [
    "# Measurement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_F1(input, true_labels, message):\n",
    "  predicted = tf.convert_to_tensor([tf.one_hot(tf.argmax(t), depth = 10) for t in classifier(input)])\n",
    "  predicted_T = predicted.numpy().T\n",
    "  true_labels_T = true_labels.T\n",
    "  total_positive = np.sum(predicted_T, axis=1)\n",
    "  true_positive = np.sum([[predicted_T[i][j] == true_labels_T[i][j] == 1 for j in range(5000)] for i in range(10)], axis = 1)\n",
    "  false_negative = np.sum([[abs(predicted_T[i][j]-1) == true_labels_T[i][j] == 1 for j in range(5000)] for i in range(10)], axis = 1)\n",
    "\n",
    "  precision = np.average(true_positive / total_positive)\n",
    "  recall = np.average(true_positive / (true_positive + false_negative))\n",
    "  F_1 = 2 * ((precision * recall) / (precision + recall))\n",
    "  print(message)\n",
    "  print(\"Precision:\", precision)\n",
    "  print(\"Recall:\", recall)\n",
    "  print(\"F1:\", F_1)\n",
    "\n",
    "def accuracy(input, true_labels, message):\n",
    "  predicted = tf.convert_to_tensor([tf.one_hot(tf.argmax(t), depth = 10) for t in classifier(input)])\n",
    "  equal = [ (tf.argmax(predicted[i]) == tf.argmax(true_labels[i])).numpy() for i in range(len(predicted))]\n",
    "  print(message, sum(equal)/len(equal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-reproduction",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "usual-facial",
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.CategoricalCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
    "EPOCHS = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bibliographic-courtesy",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedicated-sheffield",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(images, true_labels):\n",
    "  with tf.GradientTape() as disc_tape:\n",
    "    output = classifier(images)\n",
    "    loss = cross_entropy(true_labels, output)\n",
    "  \n",
    "  gradients = disc_tape.gradient(loss, parameters)\n",
    "  optimizer.apply_gradients(zip(gradients, parameters))\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "  if(e + 1 % 10  == 0):\n",
    "    calculate_F1(X_validation, Y_validation, \"Validation set Measurements: \")\n",
    "  loss_accumulator = 0\n",
    "  for i, batch in enumerate(train):\n",
    "    loss_accumulator += train_step(batch[0], batch[1])\n",
    "  clear_output(wait=True)\n",
    "  print(\"Epochs: \" + str(e+1) + \"\\\\\" + str(EPOCHS))\n",
    "  print(\"Loss: \", loss_accumulator.numpy()/len(train))\n",
    "  loss_accumulator = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-extraction",
   "metadata": {},
   "source": [
    "# Measurement for Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "consolidated-administrator",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(X_test, Y_test, \"Accuracy:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-flavor",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_F1(X_test, Y_test, \"Test set Measurements:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-trading",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-syria",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_weights = [w.numpy() for w in weights]\n",
    "np_biases = [w.numpy() for w in biases]\n",
    "np.save('./Classifier_params/weights.npy', np_weights)\n",
    "np.save('./Classifier_params/biases.npy', np_biases)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
