{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "handmade-skating",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network\n",
    "by Búgvi Benjamin Magnussen and Nikolaj Bläser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "federal-stopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dying-tours",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Swap this boolean to choose between convolutional networks and dense (maxout) networks\n",
    "use_convolutions = True\n",
    "tf.config.experimental.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-offset",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ruled-thomas",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist.load_data()\n",
    "(X_train, y_train), (X_test, y_test) = mnist\n",
    "y_train = tf.keras.utils.to_categorical(y_train)\n",
    "y_test = tf.keras.utils.to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "violent-patrick",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(images):\n",
    "  return (images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "decimal-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = normalize(X_train)\n",
    "X_test = normalize(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "satellite-nigeria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2ce8b5755b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image = X_train[0]\n",
    "plt.imshow(image, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "intellectual-drama",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = image.shape[0]\n",
    "IMAGE_HEIGHT = image.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "indirect-mandate",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "X_train = tf.data.Dataset.from_tensor_slices(X_train).shuffle(len(X_train)).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conditional-yorkshire",
   "metadata": {},
   "source": [
    "# Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code is from tensorflow.contrib.layers.maxout\n",
    "def maxout(inputs, num_units, axis=-1):\n",
    "  '''\n",
    "  inputs: Tensor input\n",
    "  num_units: The num of unit keeped after amxout\n",
    "  axis: The dimension max op performed\n",
    "  scope: Optional scope for variable_scope\n",
    "     Note: This is a slightly modified version. Replaced some unused API functions\n",
    "  '''\n",
    "  shape = inputs.get_shape().as_list()\n",
    "  num_channels = shape[axis]\n",
    "  if num_channels % num_units:\n",
    "    raise ValueError('number of features({}) is not '\n",
    "                      'a multiple of num_units({})'.format(\n",
    "                          num_channels, num_units))\n",
    "  shape[axis] = -1\n",
    "  shape += [num_channels // num_units]\n",
    "\n",
    "  # Dealing with batches with arbitrary sizes\n",
    "  for i in range(len(shape)): # This is used to handle the case where None is included in the shape\n",
    "    if shape[i] is None:\n",
    "      shape[i] = tf.shape(inputs)[i]\n",
    "  outputs = tf.reduce_max( tf.reshape(inputs, shape), -1)\n",
    "  return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-prototype",
   "metadata": {},
   "source": [
    "# Layer Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-arabic",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def denseRelu(inputs, weights, bias):\n",
    "  return tf.nn.relu(tf.nn.bias_add(tf.matmul(inputs, weights), bias))\n",
    "\n",
    "def denseLeakyRelu(inputs, weights, bias):\n",
    "  return tf.nn.leaky_relu(tf.nn.bias_add(tf.matmul(inputs, weights), bias))\n",
    "\n",
    "def denseTanh(inputs, weights, bias):\n",
    "  return tf.nn.tanh(tf.nn.bias_add(tf.matmul(inputs, weights), bias))\n",
    "\n",
    "def denseSigmoid(inputs, weights, bias):\n",
    "  return tf.nn.sigmoid(tf.nn.bias_add(tf.matmul(inputs, weights), bias))\n",
    "\n",
    "\n",
    "def denseMaxout(inputs, weights, bias, num_of_units=2, dropout_rate=0.5):\n",
    "  z = tf.nn.bias_add(tf.matmul(inputs, weights), bias)\n",
    "  z_dropout = tf.nn.dropout(z, rate=dropout_rate)\n",
    "  return maxout(z_dropout, num_of_units)\n",
    "\n",
    "def conv2dLeakyRelu(inputs, filters, channels, width, height, stride_size, dropout_rate=0.3):\n",
    "  inputs = tf.reshape(inputs, [-1, width, height, channels])\n",
    "  z = tf.nn.leaky_relu(tf.nn.conv2d(inputs, filters, [stride_size, stride_size], padding='SAME'))\n",
    "  out = tf.nn.dropout(z, rate=dropout_rate)\n",
    "  return out\n",
    "\n",
    "def conv2d_transpose(inputs, filters, channels, width, height, output_shape, stride_size):\n",
    "  inputs = tf.reshape(inputs, [-1, width, height, channels])\n",
    "  out = tf.nn.conv2d_transpose(inputs, filters, output_shape=output_shape, strides=[1, stride_size, stride_size, 1]) #padding is same\n",
    "  return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-energy",
   "metadata": {},
   "source": [
    "# Parameter Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quantitative-disabled",
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.glorot_normal()\n",
    "bias_initializer = tf.initializers.zeros()\n",
    "\n",
    "def get_biases(n_units, name):\n",
    "  return tf.Variable(bias_initializer(n_units, dtype=tf.float32), name = name, trainable = True, dtype=tf.float32)\n",
    "\n",
    "def get_weights(shape, name):\n",
    "  return tf.Variable(initializer(shape, dtype=tf.float32), name = name, trainable = True, dtype=tf.float32)\n",
    "\n",
    "def get_filters(shape, name):\n",
    "  return tf.Variable(initializer(shape, dtype=tf.float32), name = name, trainable = True, dtype=tf.float32)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-calibration",
   "metadata": {},
   "source": [
    "# Discriminator - Maxout Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sophisticated-single",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not use_convolutions):\n",
    "  discriminator_biases = [\n",
    "    get_biases(1024, 'bias0'),\n",
    "    get_biases(512, 'bias1'),\n",
    "    get_biases(1, 'bias2')\n",
    "  ]\n",
    "\n",
    "  discriminator_weights = [\n",
    "    get_weights([image.shape[0] * image.shape[1], 1024], 'weights0'),\n",
    "    get_weights([256, 512], 'weights1'),\n",
    "    get_weights([256, 1], 'weights2'),\n",
    "  ]\n",
    "\n",
    "  discriminator_parameters = discriminator_weights + discriminator_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "searching-relations",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not use_convolutions):\n",
    "  dropout_rate = 0.5\n",
    "\n",
    "  @tf.function\n",
    "  def discriminator(x):\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    x = tf.reshape(x, shape=[x.shape[0], x.shape[1] * x.shape[2]])\n",
    "    d1 = denseMaxout(x, discriminator_weights[0], discriminator_biases[0], num_of_units=256, dropout_rate=dropout_rate)\n",
    "    d2 = denseMaxout(d1, discriminator_weights[1], discriminator_biases[1], num_of_units=256, dropout_rate=dropout_rate)\n",
    "    return denseSigmoid(d2, discriminator_weights[2], discriminator_biases[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competent-apple",
   "metadata": {},
   "source": [
    "# Generator - Dense Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-ticket",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not use_convolutions):\n",
    "  generator_biases = [\n",
    "    get_biases(256, 'bias0'),\n",
    "    get_biases(512, 'bias1'),\n",
    "    get_biases(1024, 'bias2'),\n",
    "    get_biases(image.shape[0] * image.shape[1], 'bias3')\n",
    "  ]\n",
    "\n",
    "  generator_weights = [\n",
    "    get_weights([100, 256], 'weights0'),\n",
    "    get_weights([256, 512], 'weights1'),\n",
    "    get_weights([512, 1024], 'weights2'),\n",
    "    get_weights([1024, image.shape[0] * image.shape[1]], 'weights3')\n",
    "  ]\n",
    "\n",
    "  generator_parameters = generator_weights + generator_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunrise-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(not use_convolutions):\n",
    "  @tf.function\n",
    "  def generator(x):\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    d1 = denseLeakyRelu(x, generator_weights[0], generator_biases[0])\n",
    "    d2 = denseLeakyRelu(d1, generator_weights[1], generator_biases[1])\n",
    "    d3 = denseLeakyRelu(d2, generator_weights[2], generator_biases[2])\n",
    "    output = denseTanh(d3, generator_weights[3], generator_biases[3])\n",
    "    return tf.reshape(output, [x.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-powder",
   "metadata": {},
   "source": [
    "# Discriminator - Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-grill",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_convolutions):\n",
    "  discriminator_biases = [\n",
    "\n",
    "  ]\n",
    "\n",
    "  discriminator_weights = [\n",
    "    get_filters([5, 5, 1, 64], 'filters1'),\n",
    "    get_filters([5, 5, 64, 128], 'filters2'),\n",
    "    get_weights([7*7*128, 1], 'weights0') \n",
    "  ]\n",
    "\n",
    "  discriminator_parameters = discriminator_weights + discriminator_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sharp-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_convolutions):\n",
    "  dropout_rate = 0.3\n",
    "\n",
    "  @tf.function\n",
    "  def discriminator(x):\n",
    "    batchsize = x.shape[0]\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    d1 = conv2dLeakyRelu(x, discriminator_weights[0], channels=1, width=28, height=28, stride_size=2, dropout_rate=dropout_rate)\n",
    "    d2 = conv2dLeakyRelu(d1, discriminator_weights[1], channels=64, width=14, height=14, stride_size=2, dropout_rate=dropout_rate)\n",
    "    d2 = tf.reshape(d2, [batchsize, -1])\n",
    "    return tf.nn.sigmoid(tf.matmul(d2, discriminator_weights[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-yemen",
   "metadata": {},
   "source": [
    "# Generator - Convolutional Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_convolutions):\n",
    "  generator_biases = [\n",
    "  ]\n",
    "\n",
    "  generator_weights = [\n",
    "    get_weights([100, 7*7*256], 'weights0'),\n",
    "    get_filters([5, 5, 128, 256], 'filters1'),      \n",
    "    get_filters([5, 5, 64, 128], 'filters0'),\n",
    "    get_filters([5, 5, 1, 64], 'filters1')   \n",
    "  ]\n",
    "\n",
    "  generator_parameters = generator_weights + generator_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-pepper",
   "metadata": {},
   "outputs": [],
   "source": [
    "if(use_convolutions):\n",
    "  @tf.function\n",
    "  def generator(x):\n",
    "    x = tf.cast(x, dtype=tf.float32)\n",
    "    d1 = tf.nn.leaky_relu(tf.matmul(x, generator_weights[0]))\n",
    "    d2 = tf.nn.leaky_relu(conv2d_transpose(inputs=d1, filters=generator_weights[1], channels=256, width=7, height=7, output_shape=(d1.shape[0], 7, 7, 128), stride_size=1))\n",
    "    d3 = tf.nn.leaky_relu(conv2d_transpose(inputs=d2, filters=generator_weights[2], channels=128, width=7, height=7, output_shape=(d2.shape[0], 14, 14, 64), stride_size=2))\n",
    "    output = tf.nn.tanh(conv2d_transpose(inputs=d3, filters=generator_weights[3], channels=64, width=14, height=14, output_shape=(d3.shape[0], 28, 28, 1), stride_size=2))\n",
    "    return tf.reshape(output, [x.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-rocket",
   "metadata": {},
   "source": [
    "# Generated Image Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-forum",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(input):\n",
    "  gen_img = generator(input)\n",
    "  f, ax = plt.subplots(1,4, figsize=(12, 48))\n",
    "  ax[0].imshow((gen_img[0]) , cmap='gray')\n",
    "  ax[1].imshow((gen_img[1]) , cmap='gray')\n",
    "  ax[2].imshow((gen_img[2]) , cmap='gray')\n",
    "  ax[3].imshow((gen_img[3]) , cmap='gray')\n",
    "  plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-salmon",
   "metadata": {},
   "source": [
    "# Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valuable-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    assert len(real_output) == len(fake_output)\n",
    "    return tf.math.reduce_mean(tf.math.reduce_sum(tf.math.log(real_output) + tf.math.log(1 - fake_output)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator_loss(fake_output):\n",
    "    return tf.math.reduce_mean(tf.math.reduce_sum(tf.math.log(1 - fake_output)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vanilla-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(4e-5, beta_1=0.9)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(-1e-4, beta_1=0.9) # negative because we want to maximise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "painted-credits",
   "metadata": {},
   "source": [
    "# Inception Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-equilibrium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://machinelearningmastery.com/how-to-implement-the-inception-score-from-scratch-for-evaluating-generated-images/\n",
    "# calculate the inception score for p(y|x)\n",
    "def calculate_inception_score(p_yx, eps=1E-16):\n",
    "  # calculate p(y)\n",
    "  p_y = np.expand_dims(p_yx.mean(axis=0), 0)\n",
    "  # kl divergence for each image\n",
    "  kl_d = p_yx * (np.log(p_yx + eps) - np.log(p_y + eps))\n",
    "  # sum over classes\n",
    "  sum_kl_d = kl_d.sum(axis=1)\n",
    "  # average over images\n",
    "  avg_kl_d = np.mean(sum_kl_d)\n",
    "  # undo the logs\n",
    "  is_score = np.exp(avg_kl_d)\n",
    "  return is_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-assembly",
   "metadata": {},
   "source": [
    "# Loading pretrained classifier weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatal-ending",
   "metadata": {},
   "outputs": [],
   "source": [
    "np_weights = np.load('./Classifier_params/weights.npy', allow_pickle=True)\n",
    "np_biases = np.load('./classifier_params/biases.npy', allow_pickle=True)\n",
    "classifier_weights = [tf.Variable(w, trainable = True, dtype=tf.float64) for w in np_weights]\n",
    "classifier_biases = [tf.Variable(w, trainable = True, dtype=tf.float64) for w in np_biases]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-measure",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-piano",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_rate = 0.5\n",
    "\n",
    "@tf.function\n",
    "def classifier(x):\n",
    "  x = tf.cast(x, dtype=tf.float64)\n",
    "  x = tf.reshape(x, shape=[x.shape[0], x.shape[1] * x.shape[2]])\n",
    "  d1 = denseMaxout(x, classifier_weights[0], classifier_biases[0], num_of_units=240, dropout_rate=dropout_rate)\n",
    "  d2 = denseMaxout(d1, classifier_weights[1], classifier_biases[1], num_of_units=240, dropout_rate=dropout_rate)\n",
    "  return tf.nn.softmax(tf.nn.bias_add(tf.matmul(d2, classifier_weights[2]), classifier_biases[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-friday",
   "metadata": {},
   "source": [
    "# Inception Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beautiful-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inception_plot(epochs, input):\n",
    "  plt.plot(epochs, input)\n",
    "  plt.xlabel('Epochs')\n",
    "  plt.ylabel('Inception Score')\n",
    "  plt.pause(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocational-organic",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-appliance",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 500\n",
    "K = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-prototype",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-release",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(real_images, index):\n",
    "  generated_images = generator(tf.random.normal(shape = [len(real_images), 100]))\n",
    "  with tf.GradientTape() as disc_tape:\n",
    "    real_output = discriminator(real_images)\n",
    "    fake_output = discriminator(generated_images)\n",
    "    \n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "  \n",
    "  gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator_parameters)\n",
    "  discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator_parameters))\n",
    "\n",
    "  if (index % K == 0):\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "      generated_images = generator(tf.random.normal(shape = [BATCH_SIZE, 100]))\n",
    "      fake_output = discriminator(generated_images)\n",
    "      gen_loss = generator_loss(fake_output)\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator_parameters)\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator_parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "animated-debut",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "test_input = tf.random.normal([4, 100], seed=12345)\n",
    "# used for inception score\n",
    "inception_validation_input = tf.random.normal([10000, 100])\n",
    "inception_scores = [0] * EPOCHS\n",
    "epochs = list(range(0, EPOCHS))\n",
    "\n",
    "for e in range(EPOCHS):\n",
    "  clear_output(wait=True)\n",
    "  print(\"Epochs: \" + str(e+1) + \"\\\\\" + str(EPOCHS))\n",
    "  generate_image(test_input)\n",
    "  inception_scores[e] = calculate_inception_score(classifier(generator(inception_validation_input)).numpy())\n",
    "  generate_inception_plot(epochs, inception_scores)\n",
    "  plt.clf()\n",
    "  for index, real_images in enumerate(X_train):\n",
    "    train_step(real_images, index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-playlist",
   "metadata": {},
   "source": [
    "# Inception Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "following-palestine",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_classified_images = classifier(generator(tf.random.normal([10000, 100])))\n",
    "print(calculate_inception_score(generated_classified_images.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-fisher",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-GPU",
   "language": "python",
   "name": "tf-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
